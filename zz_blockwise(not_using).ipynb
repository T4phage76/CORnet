{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper module for flattening input tensor to 1-D for the use in Linear modules\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper module that stores the current tensor. Useful for accessing by name\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class CORblock_S(nn.Module):\n",
    "\n",
    "    scale = 4  # scale of the bottleneck convolution channels\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, times=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.times = times\n",
    "\n",
    "        self.conv_input = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.skip = nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=1, stride=2, bias=False)\n",
    "        self.norm_skip = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(out_channels, out_channels * self.scale,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.nonlin1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels * self.scale, out_channels * self.scale,\n",
    "                               kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.nonlin2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_channels * self.scale, out_channels,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.nonlin3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.output = Identity()  # for an easy access to this block's output\n",
    "\n",
    "        # need BatchNorm for each time step for training to work well\n",
    "        for t in range(self.times):\n",
    "            setattr(self, f'norm1_{t}', nn.BatchNorm2d(out_channels * self.scale))\n",
    "            setattr(self, f'norm2_{t}', nn.BatchNorm2d(out_channels * self.scale))\n",
    "            setattr(self, f'norm3_{t}', nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.conv_input(inp)\n",
    "\n",
    "        for t in range(self.times):\n",
    "            if t == 0:\n",
    "                skip = self.norm_skip(self.skip(x))\n",
    "                self.conv2.stride = (2, 2)\n",
    "            else:\n",
    "                skip = x\n",
    "                self.conv2.stride = (1, 1)\n",
    "\n",
    "            x = self.conv1(x)\n",
    "            x = getattr(self, f'norm1_{t}')(x)\n",
    "            x = self.nonlin1(x)\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            x = getattr(self, f'norm2_{t}')(x)\n",
    "            x = self.nonlin2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = getattr(self, f'norm3_{t}')(x)\n",
    "\n",
    "            x += skip\n",
    "            x = self.nonlin3(x)\n",
    "            output = self.output(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "def V1_model():\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "        ('V1', nn.Sequential(OrderedDict([  # this one is custom to save GPU memory\n",
    "            ('conv1', nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                            bias=False)),\n",
    "            ('norm1', nn.BatchNorm2d(64)),\n",
    "            ('nonlin1', nn.ReLU(inplace=True)),\n",
    "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "            ('conv2', nn.Conv2d(64, 512, kernel_size=3, stride=1, padding=1,\n",
    "                            bias=False)),\n",
    "            ('norm2', nn.BatchNorm2d(64)),\n",
    "            ('nonlin2', nn.ReLU(inplace=True)),\n",
    "            ('output', Identity())\n",
    "        ]))),\n",
    "        ('decoder', nn.Sequential(OrderedDict([\n",
    "            ('avgpool', nn.AdaptiveAvgPool2d(1)),\n",
    "            ('flatten', Flatten()),\n",
    "            ('linear', nn.Linear(512, 1000)),\n",
    "            ('output', Identity())\n",
    "        ])))\n",
    "    ]))\n",
    "\n",
    "    # weight initialization\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        # nn.Linear is missing here because I originally forgot \n",
    "        # to add it during the training of this network\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, time, glob, pickle, subprocess, shlex, io, pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import fire\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "Image.warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_path DATA_PATH [-o OUTPUT_PATH]\n",
      "                             [--model {Z,R,RT,S}] [--times TIMES]\n",
      "                             [--ngpus NGPUS] [-j WORKERS] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--step_size STEP_SIZE] [--momentum MOMENTUM]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ImageNet Training')\n",
    "parser.add_argument('--data_path', required=True,\n",
    "                    help='path to ImageNet folder that contains train and val folders')\n",
    "parser.add_argument('-o', '--output_path', default=None,\n",
    "                    help='path for storing ')\n",
    "parser.add_argument('--model', choices=['Z', 'R', 'RT', 'S'], default='Z',\n",
    "                    help='which model to train')\n",
    "parser.add_argument('--times', default=5, type=int,\n",
    "                    help='number of time steps to run the model (only R model)')\n",
    "parser.add_argument('--ngpus', default=0, type=int,\n",
    "                    help='number of GPUs to use; 0 if you want to run on CPU')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int,\n",
    "                    help='number of data loading workers')\n",
    "parser.add_argument('--epochs', default=20, type=int,\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--batch_size', default=256, type=int,\n",
    "                    help='mini-batch size')\n",
    "parser.add_argument('--lr', '--learning_rate', default=.1, type=float,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--step_size', default=10, type=int,\n",
    "                    help='after how many epochs learning rate should be decreased 10x')\n",
    "parser.add_argument('--momentum', default=.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight_decay', default=1e-4, type=float,\n",
    "                    help='weight decay ')\n",
    "\n",
    "\n",
    "FLAGS, FIRE_FLAGS = parser.parse_known_args()\n",
    "\n",
    "\n",
    "def set_gpus(n=1):\n",
    "    \"\"\"\n",
    "    Finds all GPUs on the system and restricts to n of them that have the most\n",
    "    free memory.\n",
    "    \"\"\"\n",
    "    gpus = subprocess.run(shlex.split(\n",
    "        'nvidia-smi --query-gpu=index,memory.free,memory.total --format=csv,nounits'), check=True, stdout=subprocess.PIPE).stdout\n",
    "    gpus = pandas.read_csv(io.BytesIO(gpus), sep=', ', engine='python')\n",
    "    gpus = gpus[gpus['memory.total [MiB]'] > 10000]  # only above 10 GB\n",
    "    if os.environ.get('CUDA_VISIBLE_DEVICES') is not None:\n",
    "        visible = [int(i)\n",
    "                   for i in os.environ['CUDA_VISIBLE_DEVICES'].split(',')]\n",
    "        gpus = gpus[gpus['index'].isin(visible)]\n",
    "    gpus = gpus.sort_values(by='memory.free [MiB]', ascending=False)\n",
    "    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # making sure GPUs are numbered the same way as in nvidia_smi\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(\n",
    "        [str(i) for i in gpus['index'].iloc[:n]])\n",
    "\n",
    "\n",
    "if FLAGS.ngpus > 0:\n",
    "    set_gpus(FLAGS.ngpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(pretrained=False):\n",
    "    map_location = None if FLAGS.ngpus > 0 else 'cpu'\n",
    "    model = getattr(cornet, f'cornet_{FLAGS.model.lower()}')\n",
    "    if FLAGS.model.lower() == 'r':\n",
    "        model = model(pretrained=pretrained, map_location=map_location, times=FLAGS.times)\n",
    "    else:\n",
    "        model = model(pretrained=pretrained, map_location=map_location)\n",
    "\n",
    "    if FLAGS.ngpus == 0:\n",
    "        model = model.module  # remove DataParallel\n",
    "    if FLAGS.ngpus > 0:\n",
    "        model = model.cuda()\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
